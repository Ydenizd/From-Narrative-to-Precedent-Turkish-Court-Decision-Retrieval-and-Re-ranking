{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip -q install scikit-learn transformers accelerate sentencepiece\n",
        "\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/colab_notebooks\"\n",
        "MERGED_CSV = os.path.join(BASE_DIR, \"mil_pairwise_merged_all.csv\")\n",
        "assert os.path.exists(MERGED_CSV), f\"Bulunamadı: {MERGED_CSV}\"\n",
        "\n",
        "df = pd.read_csv(MERGED_CSV)\n",
        "\n",
        "# --- Golden/weak flag (heuristic) ---\n",
        "# Golden: ev2/ev3 dolu olma ihtimali yüksek\n",
        "def is_nonempty(x):\n",
        "    return (x is not None) and (not (isinstance(x, float) and np.isnan(x))) and (str(x).strip() != \"\")\n",
        "\n",
        "df[\"is_golden\"] = (\n",
        "    df.apply(lambda r: is_nonempty(r.get(\"pos_ev2\")) or is_nonempty(r.get(\"pos_ev3\")) or\n",
        "                       is_nonempty(r.get(\"neg_ev2\")) or is_nonempty(r.get(\"neg_ev3\")), axis=1)\n",
        ").astype(int)\n",
        "\n",
        "print(\"rows:\", len(df), \"golden%:\", df[\"is_golden\"].mean(), flush=True)\n",
        "\n",
        "# Stratified split: train/valid içinde golden oranı aynı kalsın\n",
        "train_df, valid_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    stratify=df[\"is_golden\"]\n",
        ")\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "\n",
        "print(\"train rows:\", len(train_df), \"golden%:\", train_df[\"is_golden\"].mean(), flush=True)\n",
        "print(\"valid rows:\", len(valid_df), \"golden%:\", valid_df[\"is_golden\"].mean(), flush=True)\n",
        "\n",
        "# is_golden kolonu training dataset'e girmesin\n",
        "train_df = train_df.drop(columns=[\"is_golden\"])\n",
        "valid_df = valid_df.drop(columns=[\"is_golden\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-TZ3npjd8Ha",
        "outputId": "53db557a-5a0f-42a9-f4db-2f3f5021fcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "rows: 10757 golden%: 0.19085246816026774\n",
            "train rows: 8605 golden%: 0.19081929110981988\n",
            "valid rows: 2152 golden%: 0.19098513011152415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "\n",
        "# ---- PATHS ----\n",
        "BASE_DIR  = \"/content/drive/MyDrive/colab_notebooks\"\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"LegalBertTurk\")\n",
        "assert os.path.exists(MODEL_DIR), f\"Bulunamadı: {MODEL_DIR}\"\n",
        "\n",
        "# ---- HYPERPARAMS (istenen) ----\n",
        "MAX_LEN  = 256\n",
        "BATCH    = 32\n",
        "GRAD_ACC = 2\n",
        "LR       = 1.2e-6\n",
        "EPOCHS   = 10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device, flush=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=1).to(device)\n",
        "\n",
        "# ---- COLS ----\n",
        "POS_COLS = [\"pos_ev1\", \"pos_ev2\", \"pos_ev3\"]\n",
        "NEG_COLS = [\"neg_ev1\", \"neg_ev2\", \"neg_ev3\"]\n",
        "\n",
        "def _clean(x):\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return \"\"\n",
        "    return str(x).strip()\n",
        "\n",
        "def _nonempty_list(xs):\n",
        "    out = []\n",
        "    for v in xs:\n",
        "        t = _clean(v)\n",
        "        if t:\n",
        "            out.append(t)\n",
        "    return out\n",
        "\n",
        "class PairwiseMILDataset(Dataset):\n",
        "    def __init__(self, df, pos_cols, neg_cols):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.pos_cols = pos_cols\n",
        "        self.neg_cols = neg_cols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.df.iloc[i]\n",
        "        q = _clean(r[\"query_text\"])\n",
        "        pos = _nonempty_list([r.get(c, \"\") for c in self.pos_cols])\n",
        "        neg = _nonempty_list([r.get(c, \"\") for c in self.neg_cols])\n",
        "\n",
        "        # dummy fallback\n",
        "        if len(pos) == 0: pos = [\"\"]\n",
        "        if len(neg) == 0: neg = [\"\"]\n",
        "\n",
        "        return {\"q\": q, \"pos\": pos, \"neg\": neg}\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_len, max_pos=3, max_neg=3):\n",
        "    pairs_q, pairs_e = [], []\n",
        "    pos_sizes, neg_sizes = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        q = item[\"q\"]\n",
        "        pos = item[\"pos\"][:max_pos]\n",
        "        neg = item[\"neg\"][:max_neg]\n",
        "\n",
        "        pos_sizes.append(len(pos))\n",
        "        neg_sizes.append(len(neg))\n",
        "\n",
        "        for ev in pos:\n",
        "            pairs_q.append(q); pairs_e.append(ev)\n",
        "        for ev in neg:\n",
        "            pairs_q.append(q); pairs_e.append(ev)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        pairs_q, pairs_e,\n",
        "        padding=True, truncation=True, max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    meta = {\n",
        "        \"pos_sizes\": torch.tensor(pos_sizes, dtype=torch.long),\n",
        "        \"neg_sizes\": torch.tensor(neg_sizes, dtype=torch.long),\n",
        "    }\n",
        "    return enc, meta\n",
        "\n",
        "def pairwise_mil_loss(logits, pos_sizes, neg_sizes):\n",
        "    scores = logits.squeeze(-1)  # [N]\n",
        "    pos_pooled, neg_pooled = [], []\n",
        "    offset = 0\n",
        "\n",
        "    for p, n in zip(pos_sizes.tolist(), neg_sizes.tolist()):\n",
        "        pos_scores = scores[offset: offset+p]; offset += p\n",
        "        neg_scores = scores[offset: offset+n]; offset += n\n",
        "        pos_pooled.append(torch.logsumexp(pos_scores, dim=0))\n",
        "        neg_pooled.append(torch.logsumexp(neg_scores, dim=0))\n",
        "\n",
        "    pos = torch.stack(pos_pooled, dim=0)\n",
        "    neg = torch.stack(neg_pooled, dim=0)\n",
        "    return F.softplus(-(pos - neg)).mean()\n",
        "\n",
        "def make_loader(df, shuffle):\n",
        "    ds = PairwiseMILDataset(df, POS_COLS, NEG_COLS)\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=0,\n",
        "        collate_fn=partial(collate_fn, tokenizer=tokenizer, max_len=MAX_LEN, max_pos=3, max_neg=3)\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_pairwise_loss_acc(model, loader):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for enc, meta in loader:\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        pos_sizes = meta[\"pos_sizes\"].to(device)\n",
        "        neg_sizes = meta[\"neg_sizes\"].to(device)\n",
        "\n",
        "        out = model(**enc)\n",
        "        loss = pairwise_mil_loss(out.logits, pos_sizes, neg_sizes)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # bag-level valid_acc: pos_score > neg_score\n",
        "        scores = out.logits.squeeze(-1)\n",
        "        offset = 0\n",
        "        pos_list, neg_list = [], []\n",
        "        for p, n in zip(pos_sizes.tolist(), neg_sizes.tolist()):\n",
        "            pos_scores = scores[offset: offset+p]; offset += p\n",
        "            neg_scores = scores[offset: offset+n]; offset += n\n",
        "            pos_list.append(torch.logsumexp(pos_scores, dim=0))\n",
        "            neg_list.append(torch.logsumexp(neg_scores, dim=0))\n",
        "\n",
        "        pos = torch.stack(pos_list, dim=0)\n",
        "        neg = torch.stack(neg_list, dim=0)\n",
        "        correct += int((pos > neg).sum().item())\n",
        "        total += int(pos.size(0))\n",
        "\n",
        "    return float(np.mean(losses)) if losses else 0.0, (correct/total) if total else 0.0\n",
        "\n",
        "# ---- nDCG@10 + MRR@10 (pairwise valid'den türetilmiş) ----\n",
        "def dcg_at_k(rels, k):\n",
        "    rels = rels[:k]\n",
        "    return sum(((2**rel - 1) / np.log2(i+2)) for i, rel in enumerate(rels))\n",
        "\n",
        "def ndcg_at_k(rels, k):\n",
        "    ideal = dcg_at_k(sorted(rels, reverse=True), k)\n",
        "    return (dcg_at_k(rels, k) / ideal) if ideal > 0 else 0.0\n",
        "\n",
        "def mrr_at_k(rels, k, rel_threshold=2):\n",
        "    for i, rel in enumerate(rels[:k], start=1):\n",
        "        if rel >= rel_threshold:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ranking_metrics_from_pairwise(model, df, K=10, max_ev=3):\n",
        "    model.eval()\n",
        "\n",
        "    qmap = {}\n",
        "    for r in df.itertuples(index=False):\n",
        "        qid = getattr(r, \"query_id\")\n",
        "        qtext = getattr(r, \"query_text\")\n",
        "\n",
        "        def put(case_id, label, evs):\n",
        "            key = (qid, qtext)\n",
        "            if key not in qmap:\n",
        "                qmap[key] = {}\n",
        "            if case_id not in qmap[key]:\n",
        "                qmap[key][case_id] = {\n",
        "                    \"label\": int(label),\n",
        "                    \"evs\": [e for e in evs if isinstance(e, str) and e.strip()]\n",
        "                }\n",
        "\n",
        "        put(getattr(r, \"pos_case_id\"), getattr(r, \"pos_label\"),\n",
        "            [getattr(r, \"pos_ev1\",\"\"), getattr(r, \"pos_ev2\",\"\"), getattr(r, \"pos_ev3\",\"\")])\n",
        "        put(getattr(r, \"neg_case_id\"), getattr(r, \"neg_label\"),\n",
        "            [getattr(r, \"neg_ev1\",\"\"), getattr(r, \"neg_ev2\",\"\"), getattr(r, \"neg_ev3\",\"\")])\n",
        "\n",
        "    ndcgs, mrrs = [], []\n",
        "\n",
        "    for (qid, qtext), cases in qmap.items():\n",
        "        items = []\n",
        "        for case_id, obj in cases.items():\n",
        "            evs = obj[\"evs\"][:max_ev]\n",
        "            if not evs:\n",
        "                continue\n",
        "\n",
        "            enc = tokenizer([qtext]*len(evs), evs, padding=True, truncation=True,\n",
        "                            max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "            enc = {k: v.to(device) for k, v in enc.items()}\n",
        "            logits = model(**enc).logits.squeeze(-1)\n",
        "            score = torch.logsumexp(logits, dim=0).item()\n",
        "\n",
        "            items.append((score, obj[\"label\"]))\n",
        "\n",
        "        if not items:\n",
        "            continue\n",
        "\n",
        "        items.sort(key=lambda x: x[0], reverse=True)\n",
        "        rels = [lbl for _, lbl in items]\n",
        "        ndcgs.append(ndcg_at_k(rels, K))\n",
        "        mrrs.append(mrr_at_k(rels, K, rel_threshold=2))\n",
        "\n",
        "    return {\n",
        "        f\"ndcg@{K}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
        "        f\"mrr@{K}\": float(np.mean(mrrs)) if mrrs else 0.0,\n",
        "        \"queries_eval\": len(ndcgs)\n",
        "    }\n",
        "\n",
        "train_loader = make_loader(train_df, shuffle=True)\n",
        "valid_loader = make_loader(valid_df, shuffle=False)\n",
        "\n",
        "print(\"train batches:\", len(train_loader), \"valid batches:\", len(valid_loader), flush=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJHosvbFd7-g",
        "outputId": "c95ad787-2066-4e01-886b-040df58450db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/colab_notebooks/LegalBertTurk and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train batches: 269 valid batches: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_DIR = os.path.join(BASE_DIR, \"reranker_runs\", \"merged_all_bs32_lr12e-6\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.0)\n",
        "\n",
        "steps_per_epoch = int(math.ceil(len(train_loader) / GRAD_ACC))\n",
        "total_steps = EPOCHS * steps_per_epoch\n",
        "\n",
        "sched = get_linear_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_warmup_steps=max(10, int(0.1 * total_steps)),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "best_loss = 1e9\n",
        "best_acc = -1.0\n",
        "best_ndcg = -1.0\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    optim.zero_grad(set_to_none=True)\n",
        "    running = []\n",
        "\n",
        "    for step, (enc, meta) in enumerate(train_loader, start=1):\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        pos_sizes = meta[\"pos_sizes\"].to(device)\n",
        "        neg_sizes = meta[\"neg_sizes\"].to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
        "            out = model(**enc)\n",
        "            loss = pairwise_mil_loss(out.logits, pos_sizes, neg_sizes) / GRAD_ACC\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        running.append(loss.item() * GRAD_ACC)\n",
        "\n",
        "        if step % GRAD_ACC == 0:\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            sched.step()\n",
        "\n",
        "    tr_loss = float(np.mean(running)) if running else 0.0\n",
        "    va_loss, va_acc = eval_pairwise_loss_acc(model, valid_loader)\n",
        "    rm = eval_ranking_metrics_from_pairwise(model, valid_df, K=10)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | train={tr_loss:.4f} | valid_loss={va_loss:.4f} | valid_acc={va_acc:.4f} | \"\n",
        "        f\"ndcg@10={rm['ndcg@10']:.4f} | mrr@10={rm['mrr@10']:.4f} | queries_eval={rm['queries_eval']}\",\n",
        "        flush=True\n",
        "    )\n",
        "\n",
        "    # best-by-loss\n",
        "    if va_loss < best_loss:\n",
        "        best_loss = va_loss\n",
        "        save_dir = os.path.join(OUT_DIR, \"best_by_valid_loss\")\n",
        "        model.save_pretrained(save_dir)\n",
        "        tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "    # best-by-acc\n",
        "    if va_acc > best_acc:\n",
        "        best_acc = va_acc\n",
        "        save_dir = os.path.join(OUT_DIR, \"best_by_valid_acc\")\n",
        "        model.save_pretrained(save_dir)\n",
        "        tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "    # best-by-ndcg\n",
        "    if rm[\"ndcg@10\"] > best_ndcg:\n",
        "        best_ndcg = rm[\"ndcg@10\"]\n",
        "        save_dir = os.path.join(OUT_DIR, \"best_by_ndcg10\")\n",
        "        model.save_pretrained(save_dir)\n",
        "        tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# last checkpoint\n",
        "model.save_pretrained(os.path.join(OUT_DIR, \"last\"))\n",
        "tokenizer.save_pretrained(os.path.join(OUT_DIR, \"last\"))\n",
        "\n",
        "print(\"DONE | best_loss:\", best_loss, \"| best_acc:\", best_acc, \"| best_ndcg@10:\", best_ndcg, flush=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAmphdUXd73I",
        "outputId": "1dd9b954-d904-46f0-b9fd-50ac8c9cd059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2023233744.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
            "/tmp/ipython-input-2023233744.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | train=0.6966 | valid_loss=0.6875 | valid_acc=0.6162 | ndcg@10=0.7070 | mrr@10=0.5075 | queries_eval=119\n",
            "Epoch 2 | train=0.6703 | valid_loss=0.6389 | valid_acc=0.6645 | ndcg@10=0.7287 | mrr@10=0.4924 | queries_eval=119\n",
            "Epoch 3 | train=0.6155 | valid_loss=0.5708 | valid_acc=0.7110 | ndcg@10=0.7673 | mrr@10=0.5169 | queries_eval=119\n",
            "Epoch 4 | train=0.5481 | valid_loss=0.4992 | valid_acc=0.7756 | ndcg@10=0.8112 | mrr@10=0.5465 | queries_eval=119\n",
            "Epoch 5 | train=0.4701 | valid_loss=0.4204 | valid_acc=0.8271 | ndcg@10=0.8535 | mrr@10=0.5802 | queries_eval=119\n",
            "Epoch 6 | train=0.3945 | valid_loss=0.3551 | valid_acc=0.8499 | ndcg@10=0.8771 | mrr@10=0.5963 | queries_eval=119\n",
            "Epoch 7 | train=0.3397 | valid_loss=0.3177 | valid_acc=0.8676 | ndcg@10=0.8891 | mrr@10=0.6058 | queries_eval=119\n",
            "Epoch 8 | train=0.3076 | valid_loss=0.2964 | valid_acc=0.8741 | ndcg@10=0.8929 | mrr@10=0.5967 | queries_eval=119\n",
            "Epoch 9 | train=0.2898 | valid_loss=0.2841 | valid_acc=0.8806 | ndcg@10=0.8976 | mrr@10=0.6004 | queries_eval=119\n",
            "Epoch 10 | train=0.2791 | valid_loss=0.2794 | valid_acc=0.8838 | ndcg@10=0.9004 | mrr@10=0.6053 | queries_eval=119\n",
            "DONE | best_loss: 0.27938900625004487 | best_acc: 0.8838289962825279 | best_ndcg@10: 0.9003775511478387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seVVorehd7uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtF3a27Fd7mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}