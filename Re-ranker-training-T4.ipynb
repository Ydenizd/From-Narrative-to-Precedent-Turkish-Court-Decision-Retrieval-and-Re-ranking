{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14365819,"sourceType":"datasetVersion","datasetId":9173555},{"sourceId":14365820,"sourceType":"datasetVersion","datasetId":9173556},{"sourceId":14366101,"sourceType":"datasetVersion","datasetId":9173735},{"sourceId":14366103,"sourceType":"datasetVersion","datasetId":9173737},{"sourceId":686399,"sourceType":"modelInstanceVersion","modelInstanceId":520623,"modelId":534896}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:42:57.317860Z","iopub.execute_input":"2026-01-01T21:42:57.318111Z","iopub.status.idle":"2026-01-01T21:42:57.687210Z","shell.execute_reply.started":"2026-01-01T21:42:57.318077Z","shell.execute_reply":"2026-01-01T21:42:57.686356Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/config.json\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/training_args.bin\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/tokenizer.json\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/tokenizer_config.json\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/model.safetensors\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/special_tokens_map.json\n/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk/vocab.txt\n/kaggle/input/pairwise-merged-valid/mil_pairwise_merged_valid.csv\n/kaggle/input/golden-pairwise21/mil_pairwise_dataset_golden.csv\n/kaggle/input/pairwise-merged-train/mil_pairwise_merged_train.csv\n/kaggle/input/weak-pairwise100/mil_pairwise_dataset_2v1_2v0_1v0_from_feedback.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Checked the JSON content\n","metadata":{}},{"cell_type":"code","source":"# import json\n\n# path = \"/kaggle/input/feedback-json/feedback (4).jsonl\"\n# N = 5  # kaç satır görmek istiyorsan artır/azalt\n\n# items = []\n# with open(path, \"r\", encoding=\"utf-8\") as f:\n#     for i, line in enumerate(f):\n#         if i >= N:\n#             break\n#         items.append(json.loads(line))\n\n# # Notebook/terminalde okunabilir yazdırma\n# print(json.dumps(items, ensure_ascii=False, indent=2))\n\n# # İstersen dosyaya da yaz (JSON array olarak)\n# with open(\"feedback_preview.json\", \"w\", encoding=\"utf-8\") as out:\n#     json.dump(items, out, ensure_ascii=False, indent=2)\n\n# print(f\"\\nYazıldı: feedback_preview.json (ilk {N} satır)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:42:57.689098Z","iopub.execute_input":"2026-01-01T21:42:57.689540Z","iopub.status.idle":"2026-01-01T21:42:57.693257Z","shell.execute_reply.started":"2026-01-01T21:42:57.689515Z","shell.execute_reply":"2026-01-01T21:42:57.692461Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Generated files\n Golden labeled and weak labeled (LLM) data. Each row contains query_text, case_id, label, and 1-3 evidence texts (ev1/ev2/ev3).\n\n#The pairwise dataset contains approximately 10,000 pairs within the same query in the format (2 > 1, 2 > 0, 1 > 0).\n\n# How will this pairwise data be used? (brief)\n For each pair:\n\n Score the positive case’s 1-3 evidences using LegalBERT cross-encoder → aggregate into a single score with LogSumExp.\n\n Do the same for the negative case.\n\n Loss: Ensure pos_score > neg_score (pairwise ranking loss).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport math\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\n\n\n\nPOS_COLS = [\"pos_ev1\", \"pos_ev2\", \"pos_ev3\"]\nNEG_COLS = [\"neg_ev1\", \"neg_ev2\", \"neg_ev3\"]\n\ndef _clean(x):\n    if x is None:\n        return \"\"\n    if isinstance(x, float) and np.isnan(x):\n        return \"\"\n    return str(x).strip()\n\ndef _nonempty_list(xs):\n    return [t for t in (_clean(v) for v in xs) if t]\n\nclass MilPairwiseDataset(Dataset):\n    def __init__(self, df, require_nonempty=True):\n        self.df = df.reset_index(drop=True)\n        self.require_nonempty = require_nonempty\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        q = _clean(row.get(\"query_text\", \"\"))\n\n        pos = _nonempty_list([row.get(c, \"\") for c in POS_COLS])\n        neg = _nonempty_list([row.get(c, \"\") for c in NEG_COLS])\n\n        # Seçenek A: boş gelirse ya dummy koy, ya da örneği dışarıda bırakacak şekilde handle et.\n        # En pratik (train loop bozulmasın): dummy ekle.\n        if self.require_nonempty:\n            if len(pos) == 0:\n                pos = [\"\"]\n            if len(neg) == 0:\n                neg = [\"\"]\n\n        return {\"q\": q, \"pos\": pos, \"neg\": neg}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:42:57.694199Z","iopub.execute_input":"2026-01-01T21:42:57.694476Z","iopub.status.idle":"2026-01-01T21:43:02.911001Z","shell.execute_reply.started":"2026-01-01T21:42:57.694440Z","shell.execute_reply":"2026-01-01T21:43:02.910390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk\"\n\n# weak training set kullanıyorsan bunu train'e ver:\nTRAIN_PATH = \"/kaggle/input/pairwise-merged-train/mil_pairwise_merged_train.csv\"\nVALID_PATH = \"/kaggle/input/pairwise-merged-valid/mil_pairwise_merged_valid.csv\"\n\nMAX_LEN = 256\nBATCH = 16\nGRAD_ACC = 2          # efektif batch = BATCH * GRAD_ACC\nLR = 5e-6\nEPOCHS = 6\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:43:02.911844Z","iopub.execute_input":"2026-01-01T21:43:02.912225Z","iopub.status.idle":"2026-01-01T21:43:02.977072Z","shell.execute_reply.started":"2026-01-01T21:43:02.912200Z","shell.execute_reply":"2026-01-01T21:43:02.976434Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Cell 2: DataFrame + tokenizer\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer\n\ntrain_df = pd.read_csv(TRAIN_PATH)\nvalid_df = pd.read_csv(VALID_PATH)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_PATH,\n    local_files_only=True,\n    use_fast=True\n)\n\n# Not: _clean/_nonempty_list zaten önceki cell'de tanımlı kalsın.\n# Eğer yine de burada kalsın istiyorsan strip'li sürüm:\ndef _clean(x):\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return \"\"\n    return str(x).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:43:02.977905Z","iopub.execute_input":"2026-01-01T21:43:02.978137Z","iopub.status.idle":"2026-01-01T21:43:03.502327Z","shell.execute_reply.started":"2026-01-01T21:43:02.978114Z","shell.execute_reply":"2026-01-01T21:43:03.501705Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom functools import partial\n\nclass PairwiseMILDataset(Dataset):\n    def __init__(self, df, pos_cols, neg_cols):\n        self.df = df.reset_index(drop=True)\n        self.pos_cols = pos_cols\n        self.neg_cols = neg_cols\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        q = _clean(r[\"query_text\"])\n        pos = _nonempty_list([r.get(c, \"\") for c in self.pos_cols])\n        neg = _nonempty_list([r.get(c, \"\") for c in self.neg_cols])\n\n        # Not: pos/neg boşsa bu örnek veri kalitesi açısından sorunlu olabilir.\n        # İstersen burada \"\" koymak yerine bu satırları drop ederek dataset'i temizleyebiliriz.\n        if len(pos) == 0:\n            pos = [\"\"]\n        if len(neg) == 0:\n            neg = [\"\"]\n\n        return {\"q\": q, \"pos\": pos, \"neg\": neg}\n\n\ndef collate_fn(batch, tokenizer, max_len, max_pos=3, max_neg=3):\n    \"\"\"\n    Çıkış:\n      - enc: tokenizer output for ALL pairs (flatten edilmiş)\n      - meta: bag yapısını geri kurmak için\n          pos_sizes: (B,) her örnekte kaç pos evidence\n          neg_sizes: (B,) her örnekte kaç neg evidence\n    \"\"\"\n    pairs_q, pairs_e = [], []\n    pos_sizes, neg_sizes = [], []\n\n    for item in batch:\n        q = item[\"q\"]\n        pos = item[\"pos\"][:max_pos]\n        neg = item[\"neg\"][:max_neg]\n\n        pos_sizes.append(len(pos))\n        neg_sizes.append(len(neg))\n\n        for ev in pos:\n            pairs_q.append(q)\n            pairs_e.append(ev)\n\n        for ev in neg:\n            pairs_q.append(q)\n            pairs_e.append(ev)\n\n    enc = tokenizer(\n        pairs_q,\n        pairs_e,\n        padding=True,\n        truncation=True,\n        max_length=max_len,\n        return_tensors=\"pt\",\n    )\n\n    meta = {\n        \"pos_sizes\": torch.tensor(pos_sizes, dtype=torch.long),\n        \"neg_sizes\": torch.tensor(neg_sizes, dtype=torch.long),\n    }\n    return enc, meta\n\n\n# ---- DataLoader'lar ----\ntrain_ds = PairwiseMILDataset(train_df, POS_COLS, NEG_COLS)\nvalid_ds = PairwiseMILDataset(valid_df, POS_COLS, NEG_COLS)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=partial(collate_fn, tokenizer=tokenizer, max_len=MAX_LEN, max_pos=3, max_neg=3),\n)\n\nvalid_loader = DataLoader(\n    valid_ds,\n    batch_size=BATCH,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=partial(collate_fn, tokenizer=tokenizer, max_len=MAX_LEN, max_pos=3, max_neg=3),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:43:03.503335Z","iopub.execute_input":"2026-01-01T21:43:03.503693Z","iopub.status.idle":"2026-01-01T21:43:03.518586Z","shell.execute_reply.started":"2026-01-01T21:43:03.503656Z","shell.execute_reply":"2026-01-01T21:43:03.517871Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH, local_files_only=True, num_labels=1\n).to(device)\n\ndef pairwise_mil_loss(logits, pos_sizes, neg_sizes):\n    \"\"\"\n    logits: [N, 1] (flatten edilmiş tüm (q,ev) çiftleri)\n    pos_sizes: [B]\n    neg_sizes: [B]\n    \"\"\"\n    scores = logits.squeeze(-1)  # [N]\n\n    pos_pooled = []\n    neg_pooled = []\n\n    offset = 0\n    for p, n in zip(pos_sizes.tolist(), neg_sizes.tolist()):\n        pos_scores = scores[offset : offset + p]\n        offset += p\n        neg_scores = scores[offset : offset + n]\n        offset += n\n\n        # Pooling: logsumexp = \"softmax pooling\"\n        pos_pooled.append(torch.logsumexp(pos_scores, dim=0))\n        neg_pooled.append(torch.logsumexp(neg_scores, dim=0))\n\n    pos = torch.stack(pos_pooled, dim=0)  # [B]\n    neg = torch.stack(neg_pooled, dim=0)  # [B]\n\n    return F.softplus(-(pos - neg)).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:43:03.519381Z","iopub.execute_input":"2026-01-01T21:43:03.519724Z","iopub.status.idle":"2026-01-01T21:43:07.760701Z","shell.execute_reply.started":"2026-01-01T21:43:03.519674Z","shell.execute_reply":"2026-01-01T21:43:07.760037Z"}},"outputs":[{"name":"stderr","text":"2026-01-01 21:43:04.125840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767303784.148611     100 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767303784.155373     100 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767303784.173621     100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303784.173644     100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303784.173647     100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767303784.173649     100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/legalbertturk/pytorch/default/1/LegalBertTurk and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# Optimizer\noptim = torch.optim.AdamW(model.parameters(), lr=1.1e-6, weight_decay=0.00)\n\n# Scheduler\nsteps_per_epoch = math.ceil(len(train_loader) / GRAD_ACC)\ntotal_steps = EPOCHS * steps_per_epoch\nsched = get_linear_schedule_with_warmup(\n    optim,\n    num_warmup_steps=max(10, int(0.1 * total_steps)),\n    num_training_steps=total_steps\n)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n\n\n@torch.no_grad()\ndef eval_metrics():\n    model.eval()\n    losses = []\n    correct = 0\n    total = 0\n\n    for enc, meta in valid_loader:\n        enc = {k: v.to(device) for k, v in enc.items()}\n        pos_sizes = meta[\"pos_sizes\"].to(device)\n        neg_sizes = meta[\"neg_sizes\"].to(device)\n\n        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n            out = model(**enc)\n\n            # Seçenek A loss\n            loss = pairwise_mil_loss(out.logits, pos_sizes, neg_sizes)\n\n            # Seçenek A pairwise accuracy: bag-level pos vs neg\n            scores = out.logits.squeeze(-1)  # [N]\n            offset = 0\n            pos_list, neg_list = [], []\n            for p, n in zip(pos_sizes.tolist(), neg_sizes.tolist()):\n                pos_scores = scores[offset: offset + p]; offset += p\n                neg_scores = scores[offset: offset + n]; offset += n\n                pos_list.append(torch.logsumexp(pos_scores, dim=0))\n                neg_list.append(torch.logsumexp(neg_scores, dim=0))\n\n            pos = torch.stack(pos_list, dim=0)  # [B]\n            neg = torch.stack(neg_list, dim=0)  # [B]\n\n        losses.append(loss.item())\n        correct += (pos > neg).sum().item()\n        total += pos.size(0)\n\n    return float(np.mean(losses)), (correct / total if total > 0 else 0.0)\n\n\nbest_loss = 1e9\nbest_acc = -1.0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    optim.zero_grad(set_to_none=True)\n    running = []\n\n    for step, (enc, meta) in enumerate(train_loader, start=1):\n        enc = {k: v.to(device) for k, v in enc.items()}\n        pos_sizes = meta[\"pos_sizes\"].to(device)\n        neg_sizes = meta[\"neg_sizes\"].to(device)\n\n        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n            out = model(**enc)\n            loss = pairwise_mil_loss(out.logits, pos_sizes, neg_sizes) / GRAD_ACC\n\n        scaler.scale(loss).backward()\n        running.append(loss.item() * GRAD_ACC)\n\n        if step % GRAD_ACC == 0:\n            scaler.step(optim)\n            scaler.update()\n            optim.zero_grad(set_to_none=True)\n            sched.step()\n\n    tr = float(np.mean(running)) if len(running) else 0.0\n    va_loss, va_acc = eval_metrics()\n\n    print(f\"Epoch {epoch} | train={tr:.4f} | valid_loss={va_loss:.4f} | valid_acc={va_acc:.4f}\")\n\n    if va_loss < best_loss:\n        best_loss = va_loss\n        model.save_pretrained(\"best_reranker_loss\")\n        tokenizer.save_pretrained(\"best_reranker_loss\")\n\n    if va_acc > best_acc:\n        best_acc = va_acc\n        model.save_pretrained(\"best_reranker_acc\")\n        tokenizer.save_pretrained(\"best_reranker_acc\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T21:43:07.761687Z","iopub.execute_input":"2026-01-01T21:43:07.762436Z","iopub.status.idle":"2026-01-01T22:13:07.402253Z","shell.execute_reply.started":"2026-01-01T21:43:07.762377Z","shell.execute_reply":"2026-01-01T22:13:07.401186Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_100/2444957873.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n/tmp/ipykernel_100/2444957873.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n/tmp/ipykernel_100/2444957873.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | train=0.6884 | valid_loss=0.6879 | valid_acc=0.5912\nEpoch 2 | train=0.6241 | valid_loss=0.6605 | valid_acc=0.5866\nEpoch 3 | train=0.5386 | valid_loss=0.6099 | valid_acc=0.6611\nEpoch 4 | train=0.4416 | valid_loss=0.5544 | valid_acc=0.7104\nEpoch 5 | train=0.3697 | valid_loss=0.5377 | valid_acc=0.7346\nEpoch 6 | train=0.3383 | valid_loss=0.5342 | valid_acc=0.7356\n","output_type":"stream"}],"execution_count":8}]}